# -*- coding: utf-8 -*-
"""Final_project baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dmctxcies4n2wiNH2J9j2u7FfDbFoP_X
"""

from glob import glob
import torch
import os
import cv2
import matplotlib.pyplot as plt
from PIL import ImageFont, ImageDraw, Image
import xml
import xml.etree.ElementTree as ET
from google.colab import drive

drive.mount('/content/drive')

img_root = '/content/drive/MyDrive/New_sample/images/'
annot_root = '/content/drive/MyDrive/New_sample/라벨링데이터/'

imgs = glob(img_root + '*.jpg')
annots = glob(annot_root + '*/*meta.xml')

len(annots), len(imgs)

annots[0]

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
device

"""# 데이터 시각화와 바운딩 박스 확인"""

def draw_box(image_path, annotation_path):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    tree = ET.parse(xml_file)
    root = tree.getroot()

    for annotation in root.findall('annotation'):
        file_name = annotation.find('filename').text
        for objects in annotation.findall('object'):
            x_min = int(objects.find('bndbox').findtext('xmin'))
            y_min = int(objects.find('bndbox').findtext('ymin'))
            x_max = int(objects.find('bndbox').findtext('xmax'))
            y_max = int(objects.find('bndbox').findtext('ymax'))
            cv2.rectangle(image, (x_min, y_min), (x_max,y_max), (0,255,0), 5)
            cv2.putText(image, str(file_name),(x_min, y_min -5), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 10)

    plt.figure(figsize=(25,25))
    plt.imshow(image)
    plt.show()

img_dir = '/content/drive/MyDrive/New_sample/원천데이터/10060_해태포키블루베리41G/10060_0_m_10.jpg'
xml_file = '/content/drive/MyDrive/New_sample/라벨링데이터/10060_해태포키블루베리41G/10060_0_m_10_meta.xml'

draw_box(img_dir, xml_file)

"""# MMdetection 사용을 위한 가지고 있는 데이터셋 COCODataset의 json 파일 만들기"""

coco_dataset = {
"info": {},
"licenses": [],
"images": [],
"annotations": [],
"categories": []
}

info_dict = {
"year": int(2023),
"version": '',
"description": 'playdata final project',
"contributor": '',
"url": '',
"date_created": ''
}
coco_dataset['info'].update(info_dict)

license_dict = {
        "id": 0,
         "name": " ",
         "url": " ",
}
coco_dataset['licenses'].append(license_dict)

image_id = 0
annotation_id = 0
category_id = 0
category_mapping = {}

for annot in annots:
    tree = ET.parse(annot)
    root = tree.getroot()
    for div in root.findall('div_cd'):
        superpercategory = div.findtext('div_l')
        snack_name = div.findtext('img_prod_nm')
        if snack_name not in category_mapping.keys():
            category_mapping[snack_name] = category_id
            category_id += 1
        else:
            pass

    categories_dict = {
        "id": category_mapping[snack_name],
        "name": snack_name,
        "supercategory": superpercategory
    }
    if categories_dict not in coco_dataset['categories']:
        coco_dataset['categories'].append(categories_dict)

    # coco_dataset 중 image 카테고리 만들기

    annotations = root.findall('annotation')
    for annotation in annotations:
        file_name = annotation.find('filename').text
        t_sizes = annotation.findall('size')
        for t_size in t_sizes:
            img_width = int(t_size.findtext('width'))
            img_height = int(t_size.findtext('height'))

            image_dict = {
                "id": image_id,
                "file_name": file_name,
                "width": img_width,
                "height": img_height,
                'license': 0
            }
            coco_dataset['images'].append(image_dict)

        for obj in annotation.findall('object'):
            bbox = obj.find('bndbox')
            x_min = int(bbox.findtext('xmin'))
            y_min = int(bbox.findtext('ymin'))
            x_max = int(bbox.findtext('xmax'))
            y_max = int(bbox.findtext('ymax'))


            annotation_dict = {
                "id": annotation_id,
                "image_id": image_id,
                "category_id": category_mapping[snack_name],
                "bbox": [x_min, y_min, x_max - x_min, y_max - y_min],
                "area": (x_max - x_min) * (y_max - y_min),
                "iscrowd": 0
            }
            coco_dataset["annotations"].append(annotation_dict)

            annotation_id += 1

        image_id += 1

import json

sample_json = '/content/drive/MyDrive/New_sample/final.json'
with open(sample_json, 'w', encoding='utf-8') as json_file:
    json.dump(coco_dataset, json_file, ensure_ascii=False)

"""# MMdetection x2.0 사용을 위한 버전 맞춰주기"""

!pip install torch==1.13.0+cu116 torchvision==0.14.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116

!pip3 install openmim
!mim install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html

# Commented out IPython magic to ensure Python compatibility.
# mmdetection 2.x branch 로 설치
!git clone --branch 2.x https://github.com/open-mmlab/mmdetection.git
# %cd mmdetection
!pip install -e .

import mmdet
print(mmdet.__version__)

!mim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest .

from mmdet.apis import init_detector, inference_detector

config_file = 'mmdetection/yolov3_mobilenetv2_320_300e_coco.py'
checkpoint_file = 'mmdetection/yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth'
model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'

"""# COCO Fomat으로 변환하기"""

import mmcv
from mmdet.datasets.coco import CocoDataset
from mmdet.datasets.builder import DATASETS

@DATASETS.register_module(force=True)
class Detection(CocoDataset):
    CLASSES = ('롯데)자일리톨베타비타D용기86G', '농심오징어집83G', '꼬깔콘고소한맛72G', '농심바나나킥75G', '크라운)콘초66G', '해태포키블루베리41G', '농심매운새우깡90G')

!mkdir checkpoints
!wget -c https://download.openmmlab.com/mmdetection/v2.0/yolo/yolov3_d53_mstrain-416_273e_coco/yolov3_d53_mstrain-416_273e_coco-2b60fcd9.pth \
      -O checkpoints/yolov3_d53_mstrain-416_273e_coco-2b60fcd9.pth

from mmcv import Config
from mmcv.utils.config import ConfigDict

cfg = Config.fromfile('mmdetection/configs/yolo/yolov3_d53_mstrain-416_273e_coco.py')
print(f'Config:\n{cfg.pretty_text}')

from mmdet.apis import set_random_seed

# Modify dataset type and path
cfg.dataset_type = 'Detection'
cfg.data_root = '/content/drive/MyDrive/New_sample/'

cfg.data.test.type = 'Detection'
cfg.data.test.data_root = '/content/drive/MyDrive/New_sample/'
cfg.data.test.ann_file = 'final.json'
cfg.data.test.img_prefix = 'images/'

cfg.data.train.type = 'Detection'
cfg.data.train.data_root = '/content/drive/MyDrive/New_sample/'
cfg.data.train.ann_file = 'final.json'
cfg.data.train.img_prefix = 'images/'

cfg.data.val.type = 'Detection'
cfg.data.val.data_root = '/content/drive/MyDrive/New_sample/'
cfg.data.val.ann_file = 'final.json'
cfg.data.val.img_prefix = 'images/'

# modify num classes of the model in box head
cfg.model.bbox_head.num_classes = 7

cfg.load_from = 'checkpoints/yolov3_d53_mstrain-416_273e_coco-2b60fcd9.pth'

# Set up working dir to save files and logs.
cfg.work_dir = '/content/drive/MyDrive/New_sample/'

cfg.optimizer.lr = 0.001/8
cfg.log_config.interval = 10

cfg.runner.max_epochs = 3

# 학습 시 Batch size 설정(단일 GPU 별 Batch size로 설정됨)
# samples_per_gpu 8 -> 4
cfg.data.samples_per_gpu = 4
cfg.data.workers_per_gpu = 2


# Change the evaluation metric since we use customized dataset.
cfg.evaluation.metric = 'bbox'
# We can set the evaluation interval to reduce the evaluation times
cfg.evaluation.interval = 15
# We can set the checkpoint saving interval to reduce the storage cost
cfg.checkpoint_config.interval = 15

# Set seed thus the results are more reproducible
cfg.seed = 0
set_random_seed(0, deterministic=False)
cfg.device = 'cuda'
cfg.gpu_ids = range(1)


cfg.log_config.hooks = [
    dict(type='TextLoggerHook'),
    dict(type='TensorboardLoggerHook')]

print(f'Config:\n{cfg.pretty_text}')

from mmdet.datasets import build_dataset

train_dataset, test_dataset = [build_dataset(cfg.data.train), build_dataset(cfg.data.test)]

train_dataset

train_dataset.data_infos

train_dataset.get_ann_info

from mmdet.models import build_detector

# Build the detector
model = build_detector(cfg.model)
# Add an attribute for visualization convenience
model.CLASSES = train_dataset.CLASSES

from mmdet.apis import train_detector
import os.path as osp

# Create work_dir
mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))
train_detector(model, train_dataset, cfg, distributed=False, validate=True)

from mmdet.apis import show_result_pyplot

img = mmcv.imread('/content/drive/MyDrive/New_sample/images/10060_0_m_10.jpg') # BGR Image 사용

# checkpoint 저장된 model 파일을 이용하여 모델을 생성, 이때 Config는 위에서 update된 config 사용.
checkpoint_file = '/content/drive/MyDrive/pet_log/epoch_15.pth'
model_ckpt = init_detector(cfg, checkpoint_file, device='cuda:0')

result = inference_detector(model_ckpt, img)
show_result_pyplot(model_ckpt, img, result, score_thr=0.3)